{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trip_Advisor_Hotel_Reviews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOG/3n7yHD8NJH0RLykj0AL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HassanJoumaa/Trip_Advisor_Hotel_Reviews/blob/main/Trip_Advisor_Hotel_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67LYwkgc7ffz"
      },
      "source": [
        "# Trip Advisor Hotel Reviews\r\n",
        "Hotels play a crucial role in traveling and with the increased access to information new pathways of selecting the best ones emerged.\r\n",
        "\r\n",
        "With this dataset, consisting of 20k reviews crawled from Tripadvisor, you can explore what makes a great hotel and maybe even use this model in your travels!\r\n",
        "\r\n",
        "## 1. Problem\r\n",
        "\r\n",
        "We are provided with this dataset, consisting of **20k** reviews crawled from Tripadvisor. Each review has its corresponding rating ***(1-5)***. The goal is to develop a Sequence model capable of classifying the reviews but into **3** classes instead of **5**. \r\n",
        "\r\n",
        "## 2. Data\r\n",
        "\r\n",
        "The data we're using is from Kaggle's Trip Advisor Hotel Reviews dataset.\r\n",
        "\r\n",
        "https://www.kaggle.com/andrewmvd/trip-advisor-hotel-reviews\r\n",
        "\r\n",
        "## 3. Evaluation\r\n",
        "\r\n",
        "We will evaluate the model based on the accuracy metric, making sure that it doesn't have **High Variance** or **High Bias**.\r\n",
        "\r\n",
        "\r\n",
        "## 4. Features\r\n",
        "* There are 5 ratings ***1*** to ***5*** which we will have to change to 3 classes ***Bad, Good, and Neutral 1 to 3***.\r\n",
        "* There are around ***20k*** reviews which we will split into training and testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixlEX3hu_RIZ"
      },
      "source": [
        "### Getting the Data and Importing the Libraries\r\n",
        "We will start of by getting the data from Kaggle, using the Kaggle api but will do a pip \"force install\" first in order to prevent any problems. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnOu7f-O3MDj"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ynnDLYE7qfj"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import datetime \r\n",
        "\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "print(tf.__version__) #Make sure Tensorflow 2 is imported"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ivDM1Sa_g_X"
      },
      "source": [
        "# Adding the Username and Key from the Kaggle Token Folder\r\n",
        "os.environ['KAGGLE_USERNAME']=\"hassanjoumaa\"\r\n",
        "os.environ['KAGGLE_KEY']=\"d235272b72cd0021c0b402a603c814c5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtQKY1o-_9Ap"
      },
      "source": [
        "!kaggle datasets download -d andrewmvd/trip-advisor-hotel-reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue5JBY1n_-nv"
      },
      "source": [
        "!unzip /content/trip-advisor-hotel-reviews.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RswgOxXUxovA"
      },
      "source": [
        "### Preprocessing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbtVG1cbAEOl"
      },
      "source": [
        "df = pd.read_csv(\"/content/tripadvisor_hotel_reviews.csv\")\r\n",
        "print(\"Number of records is:\",len(df))\r\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6m9eD9VATMH"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zARSEdEyyKSL"
      },
      "source": [
        "#### We will remove the stopwords from the reviews in order to reduce the data the model has to process. \r\n",
        "\r\n",
        "This does not affect the model's decision in a bad way since we are building a model for sentiment classification and not machine translation or any other use case which might need these words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-tbm0VuAg8G"
      },
      "source": [
        "STOPWORDS = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\r\n",
        "\r\n",
        "def remove_stopwords(x, stopwords=STOPWORDS):\r\n",
        "  sentence = x.split()\r\n",
        "  new_sentence=[]\r\n",
        "  for word in sentence:\r\n",
        "    if word in stopwords:\r\n",
        "      continue\r\n",
        "    else:\r\n",
        "      new_sentence.append(word)\r\n",
        "  return \" \".join(new_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdmpYw51Aj5B"
      },
      "source": [
        "df[\"Review\"] = df[\"Review\"].apply(lambda x: remove_stopwords(x))\r\n",
        "\r\n",
        "lengths = df[\"Review\"].str.split().apply(lambda x: len(x))\r\n",
        "maxlen_index = lengths.argmax()\r\n",
        "maxlen_rating = df[\"Rating\"][maxlen_index]\r\n",
        "max_len = len(df[\"Review\"][maxlen_index].split())\r\n",
        "\r\n",
        "mean = lengths.mean()\r\n",
        "median = lengths.median()\r\n",
        "\r\n",
        "print(\"Index of largest sentence:\", maxlen_index)\r\n",
        "print(\"Rating of largest sentence:\", maxlen_rating)\r\n",
        "print(\"Length of largest sentence:\", max_len)\r\n",
        "print(\"The mean Length is:\", mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyolO6LVvn7V"
      },
      "source": [
        "#### Modify the Ratings 1 is Bad, 2 is Neutral, 3 is Good"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7Q5SPGTvSDi"
      },
      "source": [
        "def modify_ratings(x):\r\n",
        "  if x==5 or x==4:\r\n",
        "    x=3\r\n",
        "    return x\r\n",
        "\r\n",
        "  elif x==1 or x==2:\r\n",
        "    x=1\r\n",
        "    return x\r\n",
        "\r\n",
        "  else:\r\n",
        "    x=2\r\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QQhIXPnv0sg"
      },
      "source": [
        "df[\"Rating\"] = df[\"Rating\"].apply(lambda x: modify_ratings(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9TSR5dxgcpB"
      },
      "source": [
        "labels = list(df[\"Rating\"])\r\n",
        "unique_ratings = np.unique(labels)\r\n",
        "lbl=LabelEncoder()\r\n",
        "labels=lbl.fit_transform(labels)\r\n",
        "labels = to_categorical(labels)\r\n",
        "print(\"Number of unique labels:\",len(unique_ratings))\r\n",
        "labels[0:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW8-0Nj2zMQo"
      },
      "source": [
        "### Preparing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFs89YTnBDdA"
      },
      "source": [
        "NUM_SAMPLES=20490 #@param{type:\"slider\", min:10000, max:20490, step:10} \r\n",
        "MAXLEN = 350\r\n",
        "EMBEDDINGS_DIM= 128\r\n",
        "vocab_size=35000\r\n",
        "trunc_type='post'\r\n",
        "padding_type='post'\r\n",
        "test_portion = 0.1\r\n",
        "BATCHE_SIZE=64\r\n",
        "\r\n",
        "df.sample(frac=1)\r\n",
        "X= np.array(df[\"Review\"])\r\n",
        "y= labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHl5z7muYuTA"
      },
      "source": [
        "sentences = X[:NUM_SAMPLES]\r\n",
        "labels = y[:NUM_SAMPLES]\r\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\r\n",
        "tokenizer.fit_on_texts(sentences)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "\r\n",
        "\r\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\r\n",
        "\r\n",
        "padded = pad_sequences(sequences, maxlen= MAXLEN, truncating=trunc_type, padding=padding_type)\r\n",
        "\r\n",
        "split = int(test_portion * NUM_SAMPLES)\r\n",
        "\r\n",
        "test_padded = padded[0:split]\r\n",
        "train_padded = padded[split:NUM_SAMPLES]\r\n",
        "test_labels = labels[0:split]\r\n",
        "train_labels = labels[split:NUM_SAMPLES]\r\n",
        "\r\n",
        "\r\n",
        "print(test_padded.shape)\r\n",
        "print(train_padded.shape)\r\n",
        "print(test_labels.shape)\r\n",
        "print(train_labels.shape)\r\n",
        "\r\n",
        "print(train_padded[0:2],\"\\n\")\r\n",
        "print(train_labels[0:2])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC3F_mkkNzqH"
      },
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((tf.constant(train_padded),tf.constant(train_labels)))\r\n",
        "test_data = tf.data.Dataset.from_tensor_slices((tf.constant(test_padded),tf.constant(test_labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0ncQl_rPLBo"
      },
      "source": [
        "train_dataset = train_data.batch(BATCHE_SIZE)\r\n",
        "test_dataset = test_data.batch(BATCHE_SIZE)\r\n",
        "\r\n",
        "train_dataset.element_spec, test_dataset.element_spec\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYeDdpkiQ2Bv"
      },
      "source": [
        "# Uncomment to see a sample of the Data\r\n",
        "# train_sentence, train_labels = next(train_data.as_numpy_iterator())\r\n",
        "\r\n",
        "# train_sentence, train_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTMXrLHEzSdO"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt71R0WTIqk7"
      },
      "source": [
        "model = tf.keras.Sequential([\r\n",
        "          tf.keras.layers.Embedding(vocab_size, EMBEDDINGS_DIM, input_length=MAXLEN),\r\n",
        "          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\r\n",
        "          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\r\n",
        "          tf.keras.layers.Dense(64, activation='relu'),\r\n",
        "          tf.keras.layers.Dense(len(unique_ratings), activation='softmax')\r\n",
        "])\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igYDkJQxKx1S"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ul5saM-K0Hf"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\r\n",
        "%load_ext tensorboard\r\n",
        "!mkdir ./logs\r\n",
        "logdir = os.path.join(\"./logs\",\r\n",
        "                        # Make it so the logs get tracked whenever we run an experiment\r\n",
        "                        datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\r\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(logdir)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJwDn9TDfB6s"
      },
      "source": [
        "NUM_EPOCHS=5\r\n",
        "history = model.fit(x=train_dataset,\r\n",
        "                    epochs=NUM_EPOCHS,\r\n",
        "                    validation_data=test_dataset,\r\n",
        "                    callbacks=[tensorboard])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bvGykIa2gd5"
      },
      "source": [
        "%tensorboard --logdir /content/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE6CHexQ6xBD"
      },
      "source": [
        "### Creating Data to use in Tensorflow's Embedding Projector:\r\n",
        "https://projector.tensorflow.org/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_6RCR7748Oc"
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\r\n",
        "e = model.layers[0]\r\n",
        "weights = e.get_weights()[0]\r\n",
        "print(weights.shape)\r\n",
        "import io\r\n",
        "\r\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\r\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\r\n",
        "for word_num in range(1, vocab_size):\r\n",
        "  word = reverse_word_index[word_num]\r\n",
        "  embeddings = weights[word_num]\r\n",
        "  out_m.write(word + \"\\n\")\r\n",
        "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\r\n",
        "out_v.close()\r\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKcI5dcF5FvA"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.download('vecs.tsv')\r\n",
        "files.download('meta.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}